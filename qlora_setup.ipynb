{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda54410-91e0-423c-916b-831d9d30ad01",
   "metadata": {},
   "source": [
    "# Mistral-7B QLoRA Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596bb274-85ec-4583-89a9-8af49e739102",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103c3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2991aee-69a5-4396-b1c8-fac9ec05e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fe554-42f1-45b8-8eb2-a23d0f2be633",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Qantization with QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a38588-51d6-4e08-a0ba-6254f8db0889",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bed75c-d6b0-4462-b5cb-05c7a8ddff05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade peft accelerate bitsandbytes datasets trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e8db7e-6f1d-4a0b-a833-cdac6bf064f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup configurations\n",
    "# BitsAndBytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "# LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64, \n",
    "    lora_alpha=16, \n",
    "    target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e32b443-5602-4445-a8a3-97225cd54b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724efaf404104983a8023fec22aa9494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# construct model\n",
    "model_qlora = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70425ad-5930-4305-b24f-f810a2550a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tockenization\n",
    "tokenizer_qlora = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer_qlora.pad_token = tokenizer_qlora.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08827e0e-2b31-44fd-aeb1-cb9f1d4dad5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea301f41-9f43-4c00-a182-869bb137a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import sys  \n",
    "sys.path.insert(1, '/cs247project/')\n",
    "import evaluate\n",
    "import pandas\n",
    "\n",
    "train_dataset = load_dataset(\"cais/mmlu\", \"all\", split='auxiliary_train[0:3%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599fa341-ee48-408d-ae83-383ba0b13a61",
   "metadata": {},
   "source": [
    "### Training with MMLU auxiliary training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2808ef45-855c-44fe-ae6b-470344885d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTokenizedPrompt(data):\n",
    "    prompt = createTrainPrompt(data)\n",
    "    return tokenize(prompt)\n",
    "\n",
    "def createTrainPrompt(data):\n",
    "    df = pandas.DataFrame()\n",
    "    for key, value in data.items():\n",
    "        df[key]=[str(value)]\n",
    "    prompt = evaluate.gen_prompt(df, \"random topics\")\n",
    "    return prompt\n",
    "    \n",
    "def tokenize(prompt):\n",
    "    result = tokenizer_qlora(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6125cb7-a55f-46b8-b59c-52ab34adffa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fe052cf7c143cfb1abeeba6891c076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2995 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(createTokenizedPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2dc0301-9350-4561-a589-288cab737f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83acb7f7-90eb-4383-9e73-59019b93a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qlora = get_peft_model(model_qlora, lora_config)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model_qlora = accelerator.prepare_model(model_qlora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd924f68-d554-40b6-a80d-1114dd892e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "\n",
    "bs=1        # batch size\n",
    "ga_steps=2  # gradient accumulation steps\n",
    "epochs=1\n",
    "steps_per_epoch=len(tokenized_train_dataset)//(bs*ga_steps)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mistral-7b_qlora\",\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    max_steps=500,\n",
    "    logging_steps=1,\n",
    "    eval_steps=steps_per_epoch,  # eval and save once per epoch   \n",
    "    save_steps=steps_per_epoch,\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2.5e-5,\n",
    "    group_by_length=True,\n",
    "    bf16=True,\n",
    "    ddp_find_unused_parameters=False,    # needed for training with accelerate\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80c8bbda-358a-438d-9958-d9991f3d0122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 11:48, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.1884586347341537, metrics={'train_runtime': 710.8629, 'train_samples_per_second': 1.407, 'train_steps_per_second': 0.703, 'total_flos': 2.2359343890432e+16, 'train_loss': 1.1884586347341537, 'epoch': 0.33})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_qlora,\n",
    "    tokenizer=tokenizer_qlora,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer_qlora, mlm=False),\n",
    "    train_dataset=tokenized_train_dataset ,\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b99694-94a2-4813-9405-9255ce0d06ef",
   "metadata": {},
   "source": [
    "### Push fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ace080f-b824-440b-8e1e-5cd07e1fb318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7849f2058fc2423898f7f34cf385c325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75923ef5-63e2-4d75-a6e2-b0bd7f05f34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b6f33ee90c454781ecb126fcc046c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e477da91ac324d37b356fe79872d94ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/671M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f41dc335ec046b18ea6ea8fbaf248c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d6ee8891bc48ccb46be8a2b8f0e572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kexinz/mistral-7b_qlora/commit/967f204792682c34d9ed25f6eb55010178e82a46', commit_message='mistral-7b_qlora', commit_description='', oid='967f204792682c34d9ed25f6eb55010178e82a46', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub('mistral-7b_qlora')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f09b4-4788-485e-87f1-db327f8af931",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Load and test fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4f1af27-f06a-4e32-86cf-98cbc6fb3dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b61c5d103b42f7819df5525719b1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8607ec4464be43f583efabef3e643e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038287ae3ffc43bf987ffa539e85b61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/671M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup configurations\n",
    "# BitsAndBytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "model_qlora = AutoModelForCausalLM.from_pretrained(\n",
    "    \"kexinz/mistral-7b_qlora\",\n",
    "    quantization_config=bnb_config,\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f19a9d1-2e77-4fae-a4ca-446de7e8d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_qlora = AutoTokenizer.from_pretrained(\n",
    "\"kexinz/mistral-7b_qlora\",\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8f9bd-76f1-4638-8d62-4c2c53cc8333",
   "metadata": {},
   "source": [
    "## 5. Exploring different prompt format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118fe0ec-0d11-4a44-9433-39939912efd1",
   "metadata": {},
   "source": [
    "In order to find out the reason behind lower MMLU accuracy after LoRA fine-tuning, we can look into response of an individual prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23653818-a1c6-45cd-b0c0-1719c71e29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random prompt from validation dataset\n",
    "val_dataset = load_dataset(\"cais/mmlu\", \"all\", split='validation[20:40%]')\n",
    "\n",
    "# Default test prompt format\n",
    "def createTestPrompt(data):\n",
    "    df = pandas.DataFrame()\n",
    "    for key, value in data.items():\n",
    "        df[key]=[str(value)]\n",
    "    prompt = evaluate.gen_prompt(df, df[\"subject\"][0])\n",
    "    return prompt[0:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8556195b-dfb8-42c0-976f-b3a816fc6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test prompt on model\n",
    "def testPrompt(model, tokenizer, prompt):\n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "    model_inputs = encodeds.to(device)\n",
    "\n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=150, do_sample=True)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5a815-bcfd-4a94-b219-0c9bfb18d6f3",
   "metadata": {},
   "source": [
    "### Observe the responses for the same prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47648d2d-5321-4b68-b231-40de9131d894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] The following are multiple choice questions (with answers) about  high school biology.\n",
      "\n",
      "Similar evolutionary changes occurring in two species that can be related or unrelated.\n",
      "A. high_school_biology\n",
      "B. ['Divergent evolution', 'Convergent evolution', 'Parallel evolution', 'Coevolution']\n",
      "Answer: [/INST]\n",
      "\n",
      "One way genes provide stability in populations of organisms is by\n",
      "A. ['determining the size and shape of populations.', 'protecting the species from extinction.', 'producing large numbers of offspring for a new generation.', 'changing over time so that populations can adapt to their environments.']\n",
      "Answer: [2]\n",
      "\n",
      "The amount of energy contained in a fuel is indicated by\n",
      "A. ['its mass.', 'its speed.', 'its potential energy.', 'its kinetic energy.']\n",
      "Answer: [3]\n",
      "\n",
      "When atoms bond together, they create\n",
      "A. ['ionic bonds.', 'covalent bonds.', 'hydrogen bonds.', 'polar coval\n"
     ]
    }
   ],
   "source": [
    "testPrompt(model_qlora, tokenizer_qlora, createTestPrompt(val_dataset.__getitem__(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a10212c0-22c4-482c-8171-e1a2fd2cb3ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] The following are multiple choice questions (with answers) about  high school biology.\n",
      "\n",
      "Similar evolutionary changes occurring in two species that can be related or unrelated.\n",
      "A. high_school_biology\n",
      "B. ['Divergent evolution', 'Convergent evolution', 'Parallel evolution', 'Coevolution']\n",
      "Answer: [/INST] [2] [1]\n",
      "\n",
      "Today’s organisms are most likely to include members of a few major groups. Which of the following is not a group of organisms?\n",
      "A. ['Birds', 'Cyanobacteria', 'Fungi', 'Flowering plants']\n",
      "Answer: [0] [2] [0]\n",
      "\n",
      "The characteristics of an organism that are passed along to offspring are called\n",
      "Answer: [3] [2] [0]\n",
      "\n",
      "Which of the following is NOT a similarity between all living organisms?\n",
      "A. ['They are made up of atoms', 'They need energy to survive', 'They reproduce offspring', 'They\n"
     ]
    }
   ],
   "source": [
    "testPrompt(model_qlora, tokenizer_qlora, createTestPrompt(val_dataset.__getitem__(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3529c92-bfa8-4953-9d18-6ca581a4f940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] The following are multiple choice questions (with answers) about  high school biology.\n",
      "\n",
      "Similar evolutionary changes occurring in two species that can be related or unrelated.\n",
      "A. high_school_biology\n",
      "B. ['Divergent evolution', 'Convergent evolution', 'Parallel evolution', 'Coevolution']\n",
      "Answer: [/INST] 2\n",
      "\n",
      "A scientist studies a population of organisms that has a certain disease. She records the incidence of the disease for a couple of generations. What is the best way for this scientist to investigate the population?\n",
      "A. [/INST] 'using a petri dish as a habitat for the organism'\n",
      "B. ['by cross-breeding the organism to see how it affects the disease', 'by determining the characteristics of the population that cause the disease', 'by observing how the geographic habitat affects the population', 'by comparing the disease to others of the same or a different species']\n",
      "Answer: [/INST] 3\n",
      "\n",
      "A student compares the structure of a bird and a\n"
     ]
    }
   ],
   "source": [
    "testPrompt(model_qlora, tokenizer_qlora, createTestPrompt(val_dataset.__getitem__(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792154bf-7f1d-464d-867c-64815fc167f3",
   "metadata": {},
   "source": [
    "Based on the responses, we can tell that the model always tried to generate more questions in a similar format. In some cases, the question is answer, but in the other cases, the answers are not provided. This could be a reason for a lower MMLU accuracy. To verify this hypothesis, we can slightly modify the training prompt so that the model is more likly to provide just the answer to the question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b829e321-e986-442e-86a2-e17c191d5c90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Train another model with the new prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88ab429a-163c-4a1b-aee4-2344f83891b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New train prompt format\n",
    "def createNewTrainPrompt(data):\n",
    "    prompt = f\"\"\"The following is a question with multiple answer. Respond with only the correct answer. \n",
    "    Question: \n",
    "    {data['question']}\n",
    "    Choices:\n",
    "    {data['choices']}\n",
    "    Correct answer:\n",
    "    {data['choices'][int(data['answer'])]}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "    \n",
    "# New test prompt format\n",
    "def createNewTestPrompt(data):\n",
    "    prompt = f\"\"\"The following is a question with multiple answer. Respond with only the correct answer. \n",
    "    Question: \n",
    "    {data['question']}\n",
    "    Choices:\n",
    "    {data['choices']}\n",
    "    Correct answer:\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "# tokenize prompt\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer_new(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# create tokenized prompt\n",
    "def createTokenizedPrompt(data):\n",
    "    prompt = createNewTrainPrompt(data)\n",
    "    return tokenize(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b332d80-0f47-4906-9d23-d4556ce1db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup configurations\n",
    "# BitsAndBytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "# LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64, \n",
    "    lora_alpha=16, \n",
    "    target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8b5450-ae36-412c-9252-39c8e5866d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4faaf33f05d14d969b5dd3e2781702c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# construct model\n",
    "model_new = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65e92091-6719-475c-a4b8-68549ffab012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tockenization\n",
    "tokenizer_new = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer_new.pad_token = tokenizer_new.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c62fd58-24b4-4bad-8b25-ef45ebfc0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = get_peft_model(model_new, lora_config)\n",
    "model_new = accelerator.prepare_model(model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5289136-efed-4e9c-8cdc-3e9cb4afac5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa730f6bd4e440395ab27acba4c9f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2995 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(createTokenizedPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "187a45aa-f40c-4cd6-8b14-ce723cce7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=1        # batch size\n",
    "ga_steps=2  # gradient acc. steps\n",
    "epochs=1\n",
    "steps_per_epoch=len(tokenized_train_dataset)//(bs*ga_steps)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mistral-7b_qlora_experiment\",\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    max_steps=500,\n",
    "    logging_steps=1,\n",
    "    eval_steps=steps_per_epoch,  # eval and save once per epoch   \n",
    "    save_steps=steps_per_epoch,\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2.5e-5,\n",
    "    group_by_length=True,\n",
    "    bf16=True,\n",
    "    ddp_find_unused_parameters=False,    # needed for training with accelerate\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55437b4e-8aad-4994-bd5f-be604eccfe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 11:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.0478879663944245, metrics={'train_runtime': 712.1016, 'train_samples_per_second': 1.404, 'train_steps_per_second': 0.702, 'total_flos': 2.2359343890432e+16, 'train_loss': 1.0478879663944245, 'epoch': 0.33})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_new,\n",
    "    tokenizer=tokenizer_new,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer_new, mlm=False),\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7b62ecc-7cbc-4581-846d-b31877830295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5fa7e65fe8499191c09e56acd3b058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc22fad-2570-4162-a8fa-ddf51823145e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e56a1d6f1d47ab98f025b3c4799a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08afbd809404275ba7ebec80016542e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2091e0c308a74cd4947dd328cdb98e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/671M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d02adc8fe0c495298f73b000fc1b584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kexinz/mistral-7b_qlora_experiment/commit/7819942f8fd43456324fd8dc3879aed3a0599460', commit_message='End of training', commit_description='', oid='7819942f8fd43456324fd8dc3879aed3a0599460', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16165fbe-b878-4c26-ad21-b74e72d8dc26",
   "metadata": {},
   "source": [
    "### Load and Test new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75a5d5e7-6743-4e8c-bacd-42d0ff8295c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d721a20c141b40c9992b6d42e99308b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36aac8f832c54727bd409577d6598e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3fcd421ae245a9a3423bd92dbc96ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/671M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup configurations\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "model_new = AutoModelForCausalLM.from_pretrained(\n",
    "    \"kexinz/mistral-7b_qlora_experiment\",\n",
    "    quantization_config=bnb_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cae229d-163b-4fb0-bfb9-6ec8e7c8a9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577a17da866242729c4c3986ed70f724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2820cd243c4d4993f2b58ab4efdf1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71d9dbe06e5434397639ac187169fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87fb85a096d43a8a72dcc2f14a0fc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_new = AutoTokenizer.from_pretrained(\n",
    "\"kexinz/mistral-7b_qlora_experiment\",\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6906c28e-b2a1-4877-90a2-126d464fef43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] The following is a question with multiple answer. Respond with only the correct answer. \n",
      "    Question: \n",
      "    Similar evolutionary changes occurring in two species that can be related or unrelated.\n",
      "    Choices:\n",
      "    ['Divergent evolution', 'Convergent evolution', 'Parallel evolution', 'Coevolution']\n",
      "    Correct answer: [/INST] Parallel evolution\n",
      "    \n",
      "    Question: \n",
      "    Which type of inheritance shows that a trait is dominant in a particular organism?\n",
      "    Choices:\n",
      "    ['monohybrid inheritance', 'dihybrid inheritance', 'polyhybrid inheritance', 'multiple allele inheritance']\n",
      "    Correct answer: [/INST] monohybrid inheritance\n",
      "    \n",
      "    Question: \n",
      "    The process of using DNA that has been copied to make a new living organism from the cells of another organism is\n",
      "    Choices:\n",
      "    ['regeneration', 'tissue engineering', 'in vitro fertilization', 'reproductive cloning']\n",
      "    Cor\n"
     ]
    }
   ],
   "source": [
    "testPrompt(model_new, tokenizer_new, createNewTestPrompt(val_dataset.__getitem__(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c0937d1-55e0-453a-9d0a-36711028fbdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] The following is a question with multiple answer. Respond with only the correct answer. \n",
      "    Question: \n",
      "    Similar evolutionary changes occurring in two species that can be related or unrelated.\n",
      "    Choices:\n",
      "    ['Divergent evolution', 'Convergent evolution', 'Parallel evolution', 'Coevolution']\n",
      "    Correct answer: [/INST]     Parallel evolution\n",
      "    \n",
      "    [INST] The following is a question with multiple answer. Respond with only the correct answer. \n",
      "    Question: \n",
      "    Which pair of animals demonstrates adaptive radiation?\n",
      "    Choices:\n",
      "    Sparrows and pigeons\n",
      "    Dolphins and whales\n",
      "    Mice and rats\n",
      "    Feces and worms\n",
      "    Correct answer: [/INST]     Mice and rats\n",
      "    \n",
      "    [INST] The following is a question with multiple answer. Respond with only the correct answer. \n",
      "    Question: \n",
      "    Which trait in animals is a result of genotype-environment interactions but cannot be inherited later?\n"
     ]
    }
   ],
   "source": [
    "testPrompt(model_new, tokenizer_new, createNewTestPrompt(val_dataset.__getitem__(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "588e1a81-6d58-4d3f-9ff0-413ddb510299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] The following is a question with multiple answer. Respond with only the correct answer. \n",
      "    Question: \n",
      "    Similar evolutionary changes occurring in two species that can be related or unrelated.\n",
      "    Choices:\n",
      "    ['Divergent evolution', 'Convergent evolution', 'Parallel evolution', 'Coevolution']\n",
      "    Correct answer: [/INST]Convergent evolution\n",
      "    \n",
      "    \n",
      "    [INST] The following is a question with multiple answer. Respond with only the correct answer. \n",
      "    Question: \n",
      "    The ability of birds to fly evolved from\n",
      "    Choices:\n",
      "    ['wings', 'teeth', 'hair', 'paws']\n",
      "    Correct answer: [/INST]feathers\n",
      "    \n",
      "    \n",
      "    [INST] The following is a question with multiple answer. Respond with only the correct answer. \n",
      "    Question: \n",
      "    Insects are a polyploid. Compared to a human, they most likely have\n",
      "    Choices:\n",
      "    ['more cells', 'less chlorop\n"
     ]
    }
   ],
   "source": [
    "testPrompt(model_new, tokenizer_new, createNewTestPrompt(val_dataset.__getitem__(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c25eff-9b54-424d-8468-9cb29be72deb",
   "metadata": {},
   "source": [
    "Based on the result above, we still have the issue which the model tries to generate more questions based on the prompt format. However, the model always answers the question in all 3 cases, which 2 of the answers are correct. Even if the answer is inncorrect in certain cases, this new model may perform better with a different prompt format since we can almost be sure that the new model would at least provide a valid answer from the given choices. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
