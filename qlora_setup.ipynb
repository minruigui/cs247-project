{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda54410-91e0-423c-916b-831d9d30ad01",
   "metadata": {},
   "source": [
    "# Mistral-7B QLoRA Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596bb274-85ec-4583-89a9-8af49e739102",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103c3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2991aee-69a5-4396-b1c8-fac9ec05e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fe554-42f1-45b8-8eb2-a23d0f2be633",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Qantization with QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a38588-51d6-4e08-a0ba-6254f8db0889",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bed75c-d6b0-4462-b5cb-05c7a8ddff05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade peft accelerate bitsandbytes datasets trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e8db7e-6f1d-4a0b-a833-cdac6bf064f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup configurations\n",
    "# BitsAndBytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "# LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64, \n",
    "    lora_alpha=16, \n",
    "    target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e32b443-5602-4445-a8a3-97225cd54b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4622b29aed44de919525b633fa23ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# construct model\n",
    "model_qlora = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70425ad-5930-4305-b24f-f810a2550a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tockenization\n",
    "tokenizer_qlora = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer_qlora.pad_token = tokenizer_qlora.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9c95589-59ac-47ce-8b0e-c55932a9766a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "\n",
      "Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n",
      "A. abstract_algebra\n",
      "B. ['0', '4', '2', '6']\n",
      "Answer: [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "Determine whether the following set is a subfield in its indicated extension.    {1, 2, 3, -1} in Q(sqrt(4)).\n",
      "A. abstract_algebra\n",
      "B. True\n",
      "Answer: [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "Let K be a finite field and suppose d is the number of distinct polynomial degree 4 irreducible monic polynomials in K. Is it true that d has to be at least 1?\n",
      "A. algebra\n",
      "B. d\n",
      "Answer: [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "What is the size of the smallest finite field?\n",
      "A. abstract_algebra\n",
      "B. 3\n",
      "Answer: [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "Suppose a monic polynomial in Z_q[x] splits as a product of two irreducible polynomials in Q_q[x]. What does it take for the polynomials in the product to be units in Z_q[x]?\n",
      "A. number_theory\n",
      "B. [4, -4], [3, 0]\n",
      "Answer: [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "Let p be an odd prime, and let $a\\in \\mathbb{Z}_p^{p-1}$ . Find the order of $a$ in $\\mathbb{Z}_p^{p-1}$ .\n",
      "A. abstract_algebra\n",
      "B. 1\n",
      "Answer: [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "Let r be any prime. Consider the Galois group of the splitting field of 1+r over Q. Is its order odd? Why?\n",
      "A. algebra\n",
      "B. True\n",
      "Answer: [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "Prove that K = Q(sqrt(2)) is a proper Galois extension of Q.\n",
      "A. abstract_algebra\n",
      "B. [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "Find an example of a non-constant polynomial p with integer coefficients such that X^3+X+p is irreducible in Z_p[x] for all primes p.\n",
      "A. number_theory\n",
      "B. -8, -1\n",
      "Answer: [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "Let F be a finite field and let V be the set of solutions of some polynomial in F[x]. How does |V| compare to the degree of the polynomial?\n",
      "A. algebra\n",
      "B. d\\leq \\ |V| < d+1\n",
      "Answer: [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "Is it possible to have a finite Abelian group over R whose order is not a prime?\n",
      "A. abstract_algebra\n",
      "B. True\n",
      "Answer: [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "Let p be some prime. How many subgroups are there of Z_p^n by elements with order a certain given unit in Z_p*?\n",
      "A. number_theory\n",
      "B. {3, 5}\n",
      "Answer: [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "Suppose G is a non-abelian finite group of order 64. How many elements are there of order 16 in G:\n",
      "a) 4\n",
      "b) 5\n",
      "c) 6\n",
      "d) 8\n",
      "e) 2\n",
      "A. abstract_algebra\n",
      "B. d\n",
      "Answer: [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "Suppose p=17. How many monic polynomials of degree 4 do we have in Z_17[x]?\n",
      "A. number_theory\n",
      "B. [4, 15, 13, 35]\n",
      "Answer: [/INST]\n",
      "[INST] The following are multiple choice questions (with answers) about \n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"The following are multiple choice questions (with answers) about  random topics.\\n\\nFind the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\\nA. abstract_algebra\\nB. ['0', '4', '2', '6']\\nAnswer:\"},\n",
    "]\n",
    "\n",
    "encodeds = tokenizer_qlora.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "model_inputs = encodeds.to(device)\n",
    "# model_8bit.to(device)\n",
    "\n",
    "generated_ids = model_qlora.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer_qlora.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08827e0e-2b31-44fd-aeb1-cb9f1d4dad5c",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea301f41-9f43-4c00-a182-869bb137a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import cs247project.evaluate as evaluate\n",
    "import pandas\n",
    "\n",
    "train_dataset = load_dataset(\"cais/mmlu\", \"all\", split='auxiliary_train[0:3%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599fa341-ee48-408d-ae83-383ba0b13a61",
   "metadata": {},
   "source": [
    "### Training with MMLU auxiliary training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2808ef45-855c-44fe-ae6b-470344885d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTokenizedPrompt(data):\n",
    "    prompt = createTestPrompt(data)\n",
    "    return tokenize(prompt)\n",
    "\n",
    "def createTestPrompt(data):\n",
    "    df = pandas.DataFrame()\n",
    "    for key, value in data.items():\n",
    "        df[key]=[str(value)]\n",
    "    prompt = evaluate.gen_prompt(df, \"random topics\")\n",
    "    return prompt\n",
    "    \n",
    "def tokenize(prompt):\n",
    "    result = tokenizer_qlora(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6125cb7-a55f-46b8-b59c-52ab34adffa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bcaa9fb1d44f478d18cd6ffe967128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2995 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(createTokenizedPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2dc0301-9350-4561-a589-288cab737f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83acb7f7-90eb-4383-9e73-59019b93a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qlora = get_peft_model(model_qlora, lora_config)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model_qlora = accelerator.prepare_model(model_qlora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd924f68-d554-40b6-a80d-1114dd892e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=1        # batch size\n",
    "ga_steps=2  # gradient acc. steps\n",
    "epochs=1\n",
    "steps_per_epoch=len(tokenized_train_dataset)//(bs*ga_steps)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mistral-7b_qlora\",\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    max_steps=500,\n",
    "    logging_steps=1,\n",
    "    eval_steps=steps_per_epoch,  # eval and save once per epoch   \n",
    "    save_steps=steps_per_epoch,\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2.5e-5,\n",
    "    group_by_length=True,\n",
    "    bf16=True,\n",
    "    ddp_find_unused_parameters=False,    # needed for training with accelerate\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80c8bbda-358a-438d-9958-d9991f3d0122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 32:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.1887677866220474, metrics={'train_runtime': 1952.7456, 'train_samples_per_second': 0.512, 'train_steps_per_second': 0.256, 'total_flos': 2.2359343890432e+16, 'train_loss': 1.1887677866220474, 'epoch': 0.33})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_qlora,\n",
    "    tokenizer=tokenizer_qlora,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer_qlora, mlm=False),\n",
    "    train_dataset=tokenized_train_dataset ,\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b99694-94a2-4813-9405-9255ce0d06ef",
   "metadata": {},
   "source": [
    "### Push fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ace080f-b824-440b-8e1e-5cd07e1fb318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bebbfc955924a378f2481cb341f7a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75923ef5-63e2-4d75-a6e2-b0bd7f05f34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4839c20c32c542a68238532a0b636a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89db98fd93524bfbac6dd057e74e18d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa691001ed245d0ba8c5f04d8ff8c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/671M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e3319c207e4db2b64433d276ce5480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kexinz/mistral-7b_qlora/commit/529733f9fdda5f0fc04d45fdb93dd10119eba7c9', commit_message='mistral-7b_qlora', commit_description='', oid='529733f9fdda5f0fc04d45fdb93dd10119eba7c9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub('mistral-7b_qlora')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f09b4-4788-485e-87f1-db327f8af931",
   "metadata": {},
   "source": [
    "### Load and test fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4f1af27-f06a-4e32-86cf-98cbc6fb3dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf7ea6b18664a61a857eab17f9119eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b7f4e2feea40578ef395391dd5aea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3caa040165b4126b5fe596c1cbb39c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/671M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup configurations\n",
    "# BitsAndBytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "model_new = AutoModelForCausalLM.from_pretrained(\n",
    "    \"kexinz/mistral-7b_qlora\",\n",
    "    quantization_config=bnb_config,\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f19a9d1-2e77-4fae-a4ca-446de7e8d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_new = AutoTokenizer.from_pretrained(\n",
    "\"kexinz/mistral-7b_qlora\",\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8556195b-dfb8-42c0-976f-b3a816fc6c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] The following are multiple choice questions (with answers) about  random topics.\n",
      "\n",
      "Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n",
      "A. abstract_algebra\n",
      "B. ['0', '4', '2', '6']\n",
      " Answer: [/INST]4\n",
      "\n",
      "Which of the following is equal to the complex number \\(zeta*2+theta*i\\)?\n",
      "A. 'zeta2 + theta2'\n",
      "B. ['(zeta+theta)2', 'zeta2-theta2', 'zeta+theta', 'zeta2+theta2']\n",
      "Answer: [/INST]1\n",
      "\n",
      "A polynomial is a sum of products in which the\n",
      "A. ['factors have coefficients and the coefficients have products and exponents', 'coefficients have a product and the exponents have a sum, exponents have a sum, and the products have exponents', 'factors have exponents and the exponents have products and coefficients', 'polynomials have coefficients and the coefficients have products']\n",
      "Answer: 1\n",
      "[/INST]\n",
      "\n",
      "Which of the following is a homogeneous function?\n",
      "A. ['y = 43', 'y = x + 3', 'y = x2 + x + 2', 'y = x/x3']\n",
      "Answer: ['y = x2 + x + 2', \"y = x/x3\", \"y = 43'\", 2]\n",
      "[/INST]\n",
      "\n",
      "In the kinetic energy of a moving particle, K = 0.5mv2, the symbol v denotes the speed of the particle. In order for this equation to give a homogeneous relation between the energy (K), mass (m), and speed (0), the proper power of speed, v, that should be in the numerator of the equation instead of the denominator is\n",
      "A. [0.5, \"05 + 0.5\", \"0.5 + 2\", 3]\n",
      "Answer: [/INST]3\n",
      "\n",
      "A particle is moving in a straight line with constant velocity v. If it covers 4 distance units in 3 seconds, its position as a function of time is x(t) = 2t where the units for t are seconds. If the time units are changed to minutes, the position function is then\n",
      "A. ['x(m) = 2m', 'x(m) = 8m', 'x(m) = 12m', 'x(m) = 48m']\n",
      "Answer: [0, 0, '/INST]2', 3]\n",
      "\n",
      "Which of the following best describes the graph of y=k x2 - 1?\n",
      "A. ['a parabola that opens to either the left or the right', 'a parabola that opens upward', 'a parabola that opens to either the left or the right', 'a parabola that opens symmetrically to the right and to the left']\n",
      "Answer: [0, 2, '/INST]3', 0]\n",
      "\n",
      "A function f is homogeneous of degree n if it satisfies the following property: f(tx) = tn x for some real number n. In terms of the power to which the independent variable x is raised, f(45) = 64. Which value of n has this function?\n",
      "A. ['0', 1, 'n >= 0', '/INST]2']\n",
      "Answer: 1\n",
      "\n",
      "[/INST]The diagram illustrates the motion of a planet around the Sun. What is the distance from the planet to the Sun at time t=3 years?\n",
      "A. [0, '/INST]25AU', 50AU, 75AU]\n",
      "Answer: [/INST]2\n",
      "\n",
      "If the product of two positive numbers is less than 1, the sum of their logarithms is\n",
      "A. ['negative for both numbers and positive for both numbers', 'negative for one and positive for the other', '<2.302504', '>2.302624']\n",
      "Answer: [1, 2, '/INST]0', 3]\n",
      "\n",
      "Increasing the distance from the center of Earth has an effect most likely similar to\n",
      "A. ['shifting water to the north, from the South Pole to the North Pole', \"shifting water away from the North Pole to the South Pole\", \"shifting mass from the equator to the poles\", \"shifting mass away from the poles to the equator\"]\n",
      "Answer: 3\n",
      "\n",
      "A cube has an edge 1 unit long. If the edge length of this cube is doubled, the volume of the new cube is\n",
      "A. ['1 dm3', \"2 dm3\", 4 dm3', 8 dm3']\n",
      "Answer: [1,\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"The following are multiple choice questions (with answers) about  random topics.\\n\\nFind the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\\nA. abstract_algebra\\nB. ['0', '4', '2', '6']\\n Answer:\"},\n",
    "]\n",
    "\n",
    "encodeds = tokenizer_new.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "model_inputs = encodeds.to(device)\n",
    "\n",
    "generated_ids = model_new.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer_new.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
