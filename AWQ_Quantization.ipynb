{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc459a4",
   "metadata": {},
   "source": [
    "### Load the Original Mistral 7B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103c3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e8a8ad9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# load the model like this, on aws ec2 g5 family remember the dtype should be bfloat16, otherwise out of memory\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.bfloat16)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "\n",
    "# encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "# model_inputs = encodeds.to(device)\n",
    "# model.to(device)\n",
    "\n",
    "# generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "# decoded = tokenizer.batch_decode(generated_ids)\n",
    "# print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5cf52",
   "metadata": {},
   "source": [
    "## Load AWQ Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07ce359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf60587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install git+https://github.com/huggingface/transformers.git@72958fcd3c98a7afdc61f953aa58c544ebda2f79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5833e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install git+https://github.com/casper-hansen/AutoAWQ.git@1c5ccc791fa2cb0697db3b4070df1813f1736208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3f8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-v0.1-AWQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28b9d61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e89c79a8d2541b9a7e244a0f6042829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|███████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  9.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "awq_model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n",
    "                                          trust_remote_code=False, safetensors=True)\n",
    "awq_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5b4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "awq_tokenizer.pad_token = awq_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec49ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "\n",
    "# encodeds = awq_tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "# model_inputs = encodeds.to(device)\n",
    "\n",
    "# awq_model.to(device)\n",
    "\n",
    "# generated_ids = awq_model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "# decoded = awq_tokenizer.batch_decode(generated_ids)\n",
    "# print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f4ead",
   "metadata": {},
   "source": [
    "### Log in hugging face with access token in order to read/write to your repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15839b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208c81c",
   "metadata": {},
   "source": [
    "### Push the model to the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf242a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# awq_model.push_to_hub('AWQ4bit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37120b6",
   "metadata": {},
   "source": [
    "## evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98761152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate_awq\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "argument = \"--ntrain 5 --data_dir data --save_dir awq_results --model mistralai/Mistral-7B-v0.1\"\n",
    "argument = argument.split(\" \")\n",
    "parser = evaluate_awq.get_parser()\n",
    "args = parser.parse_args(argument)\n",
    "accuracy_result = evaluate_awq.main(args, pass_model=awq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd63c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df2ee271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6287719298245615\n"
     ]
    }
   ],
   "source": [
    "# get the overall accuracy\n",
    "cat.get_overall_acc(accuracy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "270b4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEM : 0.513\n",
      "humanities : 0.674\n",
      "social sciences : 0.727\n",
      "other (business, health, misc.) : 0.651\n"
     ]
    }
   ],
   "source": [
    "# get the accuracy for categories\n",
    "cat_acc = cat.get_cat_acc(accuracy_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7d69d2",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e780eb",
   "metadata": {},
   "source": [
    "# The Benchmark for memory and latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9b98f",
   "metadata": {},
   "source": [
    "### Benchmark for different batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum_benchmark.logging_utils import setup_logging\n",
    "from optimum_benchmark.experiment import launch, ExperimentConfig\n",
    "from optimum_benchmark.backends.pytorch.config import PyTorchConfig\n",
    "from optimum_benchmark.launchers.torchrun.config import TorchrunConfig\n",
    "from optimum_benchmark.benchmarks.inference.config import InferenceConfig\n",
    "\n",
    "input_shapes={\"batch_size\": 1, \"num_choices\": 2, \"seqeunce_length\": 16}\n",
    "l_ori = []\n",
    "l_8bit = []\n",
    "l_4bit = []\n",
    "\n",
    "batch_step = 5\n",
    "\n",
    "for i in range(1, 10):\n",
    "    input_shapes[\"batch_size\"] = i * 5\n",
    "    \n",
    "    try:\n",
    "        setup_logging(level=\"INFO\")\n",
    "        launcher_config = TorchrunConfig(nproc_per_node=1)\n",
    "        benchmark_config = InferenceConfig(latency=True, memory=True, input_shapes=input_shapes)\n",
    "        backend_config = PyTorchConfig(model=\"mistralai/Mistral-7B-v0.1\", device=\"cuda\", device_ids=\"0\",\n",
    "                                      torch_dtype=\"bfloat16\", quantization_scheme=\"bnb\",\n",
    "                                      quantization_config={\"load_in_4bit\": True})\n",
    "        experiment_config = ExperimentConfig(\n",
    "            experiment_name=\"api-launch\",\n",
    "            benchmark=benchmark_config,\n",
    "            launcher=launcher_config,\n",
    "            backend=backend_config,\n",
    "        )\n",
    "        benchmark_report = launch(experiment_config)\n",
    "        \n",
    "        # experiment_config.push_to_hub(\"AwAppp/benchmarks_8bit_batch_size\"+str(i * batch_step))\n",
    "        benchmark_report.push_to_hub(\"AwAppp/benchmarks_4bit_batch_size\"+str(i * batch_step))\n",
    "        \n",
    "        l_4bit += [benchmark_report.to_dict()]\n",
    "        \n",
    "    except:\n",
    "        break\n",
    "\n",
    "for i in range(1, 10):\n",
    "    input_shapes[\"batch_size\"] = i * batch_step\n",
    "    \n",
    "    try:\n",
    "        setup_logging(level=\"INFO\")\n",
    "        launcher_config = TorchrunConfig(nproc_per_node=1)\n",
    "        benchmark_config = InferenceConfig(latency=True, memory=True)\n",
    "        backend_config = PyTorchConfig(model=\"mistralai/Mistral-7B-v0.1\", device=\"cuda\", device_ids=\"0\", no_weights=True, torch_dtype='bfloat16')\n",
    "        experiment_config = ExperimentConfig(\n",
    "            experiment_name=\"api-launch\",\n",
    "            benchmark=benchmark_config,\n",
    "            launcher=launcher_config,\n",
    "            backend=backend_config,\n",
    "        )\n",
    "        benchmark_report = launch(experiment_config)\n",
    "        \n",
    "        # experiment_config.push_to_hub(\"AwAppp/benchmarks_original_batch_size\"+str(i * batch_step))\n",
    "        benchmark_report.push_to_hub(\"AwAppp/benchmark_original_batch_size\"+str(i * batch_step))\n",
    "        \n",
    "        l_ori += [benchmark_report.to_dict()]\n",
    "        \n",
    "    except:\n",
    "        break\n",
    "        \n",
    "        \n",
    "for i in range(1, 10):\n",
    "    input_shapes[\"batch_size\"] = i * 5\n",
    "    \n",
    "    try:\n",
    "        setup_logging(level=\"INFO\")\n",
    "        launcher_config = TorchrunConfig(nproc_per_node=1)\n",
    "        benchmark_config = InferenceConfig(latency=True, memory=True, input_shapes=input_shapes)\n",
    "        backend_config = PyTorchConfig(model=\"AwAppp/q8bit\", device=\"cuda\", device_ids=\"0\",\n",
    "                                      torch_dtype=\"bfloat16\", quantization_scheme=\"bnb\",\n",
    "                                      quantization_config={\"load_in_8bit\": True})\n",
    "        experiment_config = ExperimentConfig(\n",
    "            experiment_name=\"api-launch\",\n",
    "            benchmark=benchmark_config,\n",
    "            launcher=launcher_config,\n",
    "            backend=backend_config,\n",
    "        )\n",
    "        benchmark_report = launch(experiment_config)\n",
    "        \n",
    "        # experiment_config.push_to_hub(\"AwAppp/benchmarks_8bit_batch_size\"+str(i * batch_step))\n",
    "        benchmark_report.push_to_hub(\"AwAppp/benchmarks_8bit_batch_size\"+str(i * batch_step))\n",
    "        \n",
    "        l_8bit += [benchmark_report.to_dict()]\n",
    "        \n",
    "    except:\n",
    "        break\n",
    "        \n",
    "    # experiment_config.push_to_hub(\"IlyasMoutawwakil/benchmarks\") # pushes experiment_config.json to the hub\n",
    "    # benchmark_report.push_to_hub(\"IlyasMoutawwakil/benchmarks\") # pushes benchmark_report.json to the hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278dda7",
   "metadata": {},
   "source": [
    "### plot of the prefill benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87095f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the test result from hugging face\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "batch_step = 5\n",
    "\n",
    "l_ori = []\n",
    "l_4bit = []\n",
    "l_8bit = []\n",
    "\n",
    "base_repo_name = \"AwAppp/benchmark_original_batch_size\"\n",
    "\n",
    "for i in range(1, 10):\n",
    "    batch_size = i * batch_step\n",
    "    hf_hub_download(repo_id=base_repo_name+str(batch_size), filename=\"benchmark_report.json\", local_dir='./')\n",
    "    \n",
    "    with open('./benchmark_report.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        l_ori += [data]\n",
    "        \n",
    "base_repo_name = \"AwAppp/benchmarks_8bit_batch_size\"\n",
    "\n",
    "for i in range(1, 10):\n",
    "    batch_size = i * 5\n",
    "    hf_hub_download(repo_id=base_repo_name+str(batch_size), filename=\"benchmark_report.json\", local_dir='./')\n",
    "    \n",
    "    with open('./benchmark_report.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        l_8bit += [data]\n",
    "        \n",
    "base_repo_name = \"AwAppp/benchmarks_4bit_batch_size\"\n",
    "\n",
    "for i in range(1, 10):\n",
    "    batch_size = i * 5\n",
    "    hf_hub_download(repo_id=base_repo_name+str(batch_size), filename=\"benchmark_report.json\", local_dir='./')\n",
    "    \n",
    "    with open('./benchmark_report.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        l_4bit += [data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5be4f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = []\n",
    "for i in range(1, 10):\n",
    "    x += [i * batch_step]\n",
    "    \n",
    "plt.figure(figsize=(8, 10))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.xticks(range(len(x)), x)\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('prefill latency')\n",
    "prefill_latency_ori = list(map(lambda x: x['prefill']['latency']['mean'], l_ori))\n",
    "prefill_latency_8bit = list(map(lambda x: x['prefill']['latency']['mean'], l_8bit))\n",
    "prefill_latency_4bit = list(map(lambda x: x['prefill']['latency']['mean'], l_4bit))\n",
    "plt.plot(prefill_latency_ori, label='7B latency')\n",
    "plt.plot(prefill_latency_8bit, label='INT8 7B latency')\n",
    "plt.plot(prefill_latency_4bit, label='INT4 7B latency')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.xticks(range(len(x)), x)\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('prefill throughput')\n",
    "prefill_throughput_ori = list(map(lambda x: x['prefill']['throughput']['value'], l_ori))\n",
    "prefill_throughput_8bit = list(map(lambda x: x['prefill']['throughput']['value'], l_8bit))\n",
    "prefill_throughput_4bit = list(map(lambda x: x['prefill']['throughput']['value'], l_4bit))\n",
    "plt.plot(prefill_throughput_ori, label='7B throughput')\n",
    "plt.plot(prefill_throughput_8bit, label='INT8 7B throughput')\n",
    "plt.plot(prefill_throughput_4bit, label='INT4 7B throughput')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.xticks(range(len(x)), x)\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('prefill vRAM')\n",
    "prefill_vram_ori = list(map(lambda x: x['prefill']['memory']['max_vram'], l_ori))\n",
    "prefill_vram_8bit = list(map(lambda x: x['prefill']['memory']['max_vram'], l_8bit))\n",
    "prefill_vram_4bit = list(map(lambda x: x['prefill']['memory']['max_vram'], l_4bit))\n",
    "plt.plot(prefill_vram_ori, label='7B vram')\n",
    "plt.plot(prefill_vram_8bit, label='INT8 7B vram')\n",
    "plt.plot(prefill_vram_4bit, label='INT4 7B vram')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6b3d0",
   "metadata": {},
   "source": [
    "### plot of the decode benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916f0eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 10))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.xticks(range(len(x)), x)\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('decode latency')\n",
    "decode_latency_ori = list(map(lambda x: x['decode']['latency']['mean'], l_ori))\n",
    "decode_latency_8bit = list(map(lambda x: x['decode']['latency']['mean'], l_8bit))\n",
    "decode_latency_4bit = list(map(lambda x: x['decode']['latency']['mean'], l_4bit))\n",
    "plt.plot(decode_latency_ori, label='7B latency')\n",
    "plt.plot(decode_latency_8bit, label='INT8 7B latency')\n",
    "plt.plot(decode_latency_4bit, label='INT4 7B latency')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.xticks(range(len(x)), x)\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('decode throughput')\n",
    "decode_throughput_ori = list(map(lambda x: x['decode']['throughput']['value'], l_ori))\n",
    "decode_throughput_8bit = list(map(lambda x: x['decode']['throughput']['value'], l_8bit))\n",
    "decode_throughput_4bit = list(map(lambda x: x['decode']['throughput']['value'], l_4bit))\n",
    "plt.plot(decode_throughput_ori, label='7B throughput')\n",
    "plt.plot(decode_throughput_8bit, label='INT8 7B throughput')\n",
    "plt.plot(decode_throughput_4bit, label='INT4 7B throughput')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.xticks(range(len(x)), x)\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('prefill vRAM')\n",
    "decode_vram_ori = list(map(lambda x: x['decode']['memory']['max_vram'], l_ori))\n",
    "decode_vram_8bit = list(map(lambda x: x['decode']['memory']['max_vram'], l_8bit))\n",
    "decode_vram_4bit = list(map(lambda x: x['decode']['memory']['max_vram'], l_4bit))\n",
    "plt.plot(decode_vram_ori, label='7B vram')\n",
    "plt.plot(decode_vram_8bit, label='INT8 7B vram')\n",
    "plt.plot(decode_vram_4bit, label='INT4 7B vram')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
